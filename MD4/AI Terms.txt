################################################
#            Reinforcement Learning            #
################################################

Reinforcement Learning is a branch of machine learning that deals with a process of learning and controlling strategies
to interact with a complex environment. RL is essentially a framework for learning from experience based on The Agent,
The Policy, π (s,a) State s, Action a, Reward r,and Environment. 

In simple terms, Compared to humanity; its like like learning as humans do when we learn by trial and error.
Good behavior is rewarded, while bad behavior or otherwise is decreased or negative.

We start with:
1. State
2. Agent
3. Environment
4. Action
5. Policy
6. Rewards

In RL the State is essentially like a snapshot of the environment at a particular moment, this means it represents 
any relevant information that the agent requires in order to make descisions.

For example, if we consider a simple game like Tic-Tac-Toe, the state would be the current configuration of the game 
board, showing where X's and O's are placed. The state tells the agent where things currently stand in the game.

Similarly, in more complex scenarios like training an AI to play a car racing game, the state would include information
like the car's current position on the track, its speed, and the direction it's facing. It may also include details 
about the track layout and any obstacles nearby. A good example of this is illustrated by videos circuling the web of
developers utilizing AI to learn how to finish a race in the video game "TrackMania" a PC videogame that involves a 
plethora of track customization, formula one racing vehicles, timed races, etc. The computer learns to drive by commiting 
hundreads of errors and falling of the racing platform, or crashing several times, not making it in the limit of time, etc.
A positive reward is given when the car passes through a checkpoint or reaches the finish line. Negative rewards may be
given when the car goes off-track or crashes. The magnitude of the rewards can depend on factors such as speed, 
accuracy, and distance from the optimal path.

The Agent here is the vehicle itself.
The Environment is the track that is selected to learn from.

(The Agent takes certain actions to interact with the Environment it is given.)

Action is related to a variety of variables like to move forward, backward, left, right, and to also study the ammount of
threshhold or how fast to go, what direction is optimal and does not cause failure, etc.

The Policy is essentially a type of strategy utilized to achieve goals with maximum reward gains.

The Rewards in RL are real-valued and represent how well the car is performing.

This form of learning in TrackMania is known as Supervised learning.

In supervised learning, the training process involves using labeled data, where each input (state) is paired with the
corresponding correct output (action). The objective is for the model to learn a mapping from inputs to outputs based
on the provided labeled data. During training, the model tries to minimize the error between its predictions and the 
actual outputs.

In the Trackmania example, the model is trained using labeled data consisting of states and corresponding actions,
this is supervised learning. On the other hand, The Markov Decision Process (MDP) is a framework for reinforcement learning,
where the agent learns to make decisions by interacting with an environment and receiving rewards or penalties based on its actions.

The differences are:

Supervised vs. Reinforcement Learning
MDP is a reinforcement learning framework that works by having the agent learn from interactions with the environment,
whereas this type of Supervised learning works by implementing labeled data and calculating optimal actions for each state.

Model Predictions vs. Policy Learning: 
In the Trackmania example, the model predicts optimal actions directly from states.
In MDP, the agent learns a policy, which is a mapping from states to actions based on expected rewards.

Deterministic vs. Stochastic Transitions: 
In the Trackmania example, transitions between states are deterministic because the model is trained using specific actions for each state.
In MDP, transitions are probabilistic, meaning the agent's actions may lead to different states with certain probabilities

Continuous vs. Discrete Actions: 
The Trackmania example uses regression training to predict continuous action values for precise control of the car's movements.
In MDP, the agent often deals with discrete action spaces, making it more challenging to control continuous movements.


Sources
-----------------------------------------

Steve Brunton. (2021, February 12). Reinforcement Learning: Machine Learning meets control Theory [Video]. YouTube. https://www.youtube.com/watch?v=0MNVhXEX9to

Robins, A. (2021, December 22). Stochastic vs Deterministic Models. Understand the Pros and Cons. Retrieved July 26, 2023, from https://blog.ev.uk/stochastic-vs-deterministic-models-understand-the-pros-and-cons

Leonelli, M. (2021, April 19). 1.2 Types of simulations | Simulation and Modelling to Understand Change. https://bookdown.org/manuele_leonelli/SimBook/types-of-simulations.html

Das, A. (2017). The very basics of reinforcement learning. Medium. Retrieved from https://becominghuman.ai/the-very-basics-of-reinforcement-learning-154f28a79071

Beysolow, I. T. (2019). Applied reinforcement learning with python : With openai gym, tensorflow, and keras. Apress L. P..

Lamba, A. (2018). A brief introduction to reinforcement learning - We’ve moved to freeCodeCamp.org/news - Medium. Medium. https://medium.com/free-code-camp/a-brief-introduction-to-reinforcement-learning-7799af5840db







################################################
# Deterministic vs Stochastic Machine Learning #
################################################

Deterministic Model

A Deterministic Model allows you to calculate a future event exactly, without the involvement of randomness.
If something is deterministic, you have all of the data necessary to predict (determine) the outcome with certainty. 

Most financial planners will be accustomed to using some form of cash flow modelling tool powered by a deterministic
model to project future investment returns. Typically, this is due to their simplicity.

Often, a single estimate for an investment return, such as 2%, 5% or 8%, is used to predict the future value of a
fund or portfolio. This is the basis of deterministic forecasts, which produce a specific result for a specific 
input - every single time.

Deterministic models are typically used by product providers to illustrate statutory future projections of long-term
investments (such as pensions). If the same projection rates are used, these forecasts can then be used to compare
different providers, particularly around charges.

However, deterministic models do not make any allowance for the fact that markets are complex, irregular and 
ever-changing. As such, any model that is based on long-term average returns can be easily upset by the unexpected
implications of sequencing risk, which can have a huge impact on a retiree’s income and lifestyle in retirement.

 Pros:

 o - Deterministic models have the benefit of simplicity. They rely on single assumptions about long-term average
     returns and inflation.
 o - Deterministic is easier to understand and hence may be more appropriate for some customers.


Cons:

o - Cash flow modelling tools that use deterministic or over-simplistic stochastic projections are fundamentally
    flawed when making financial planning decisions because they are unable to consider ongoing variables that will
    affect the plan over time.
o - Deterministic tools tend to overestimate the level of sustainable income (on a like for like basis) because
    they are unable to take into account market volatility, which causes ‘pound cost ravaging' and sequencing risk, 
    both of which have a significant negative effect on sustainable income.
o - The choice of future increase assumption is critical and puts the responsibility of the final outcome on the 
    provider of the tool.

oo - All of which renders deterministic models inadequate and potentially misleading.


Stochastic Model

Stochastic Models, use lots of historical data to illustrate the likelihood of an event occurring, such as your 
client running out of money. These types of financial planning tools are therefore considered more sophisticated 
compared with their deterministic counterparts. A stochastic model will not produce one determined outcome, but a
range of possible outcomes, this is particularly useful when helping a customer plan for their future.

Pros:

o - Stochastic models can reflect real-world economic scenarios that provide a range of possible outcomes your
    customer may experience and the relative likelihood of each.
o - By running thousands of calculations, using many different estimates of future economic conditions, stochastic
    models predict a range of possible future investment results showing the potential upside and downsides of each.
o - A stochastic model also has the ability to avoid the significant shortfalls inherent in deterministic models,
    which gives it the edge.

Cons:

o - Managing drawdown effectively and choosing suitable investment strategies requires the ability to model
    investment risk and return realistically. The problem is that nearly all strategies and solutions are
    currently designed using an assumed fixed rate of investment return throughout retirement. This is obviously
    unrealistic and ignores the important effect that the sequence of returns and volatility has on drawdown outcomes.
o - The problem of ignoring specific risk factors not only applies with deterministic modellers, but also with a
    commonly used type of simple stochastic model - mean, variance, co-variance (MVC) models. For instance, MVC models
    provide time-independent forecasts, which means that they ignore the fact that specific investment risks change 
    over time depending on the combination of assets held within the customer’s portfolio.
    
