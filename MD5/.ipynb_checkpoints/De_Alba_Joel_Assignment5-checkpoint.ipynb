{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3f73074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space - Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32) \n",
      "\n",
      "Observation Space (High Limit) - [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38] \n",
      "\n",
      "Observation Space (Low Limit) - [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38] \n",
      "\n",
      "Action Space - Discrete(2) \n",
      "\n",
      "Specifications - EnvSpec(id='CartPole-v1', entry_point='gym.envs.classic_control.cartpole:CartPoleEnv', reward_threshold=475.0, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'render_mode': 'human'}, namespace=None, name='CartPole', version=1) \n",
      "\n",
      "# of Steps per Episode - 500 \n",
      "\n",
      "Reward Threshhold - 475.0 \n",
      "\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 153\u001b[0m\n\u001b[0;32m    151\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(random_action)\n\u001b[0;32m    152\u001b[0m appendedObservations\u001b[38;5;241m.\u001b[39mappend(observation)\n\u001b[1;32m--> 153\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (terminated):\n\u001b[0;32m    155\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Aug 12 23:57:44 2023\n",
    "  5-1 Assignment: Cartpole Problem\n",
    "            Joel De Alba\n",
    "  Southern New Hampshire University\n",
    "      Professor Timothy Alexander\n",
    "              08 / 06 / 23\n",
    "              \n",
    "     Implementation Revision 10\n",
    "     \n",
    "     States in the CartPole implementation\n",
    "     are as follows:\n",
    "         - Cart Position x (x)\n",
    "         - Cart Velocity ẋ (x Dot)\n",
    "         - Pole Angle \tθ (Theta - Angle of Rotation)\n",
    "         - Pole Velocity θ (Theta Dot - Angular Velocity )\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\"\"\" \n",
    "render_mode = 'human' provides for\n",
    "a visual rendering animation of \n",
    "the CartPole Problem.\n",
    "\n",
    "In this first code we are creating the\n",
    "initial learning environment.\n",
    "\"\"\"\n",
    "env = gym.make('CartPole-v1', render_mode = 'human')\n",
    "\n",
    "# Define exploration parameters\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.995\n",
    "\n",
    "# Define other parameters\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.01  # Experiment with different learning rates\n",
    "\n",
    "# Initialize Q-table\n",
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.shape[0]\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Other Practice Modifications\n",
    "\n",
    "# Define exploration parameters\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.995\n",
    "\n",
    "# Define other parameters\n",
    "GAMMA = 0.9  # Experiment with different discount factors\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Initialize Q-table\n",
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.shape[0]\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "\n",
    "---------------------------------------------------------\n",
    "\n",
    "# Define exploration parameters\n",
    "EXPLORATION_MAX = 1.0  # Initial exploration probability\n",
    "EXPLORATION_MIN = 0.01  # Minimum exploration probability\n",
    "EXPLORATION_DECAY = 0.995  # Decay rate of exploration probability\n",
    "\n",
    "# Define other parameters\n",
    "GAMMA = 0.99  # Discount factor\n",
    "LEARNING_RATE = 0.001  # Learning rate\n",
    "\n",
    "# Initialize Q-table\n",
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.shape[0]\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Resets the environment and returns\n",
    "to the initial state.\n",
    "\"\"\"\n",
    "(state, initial_state) = env.reset()\n",
    "\n",
    "# Rendering the Environment\n",
    "env.render()\n",
    "\n",
    "# Closing the Environment\n",
    "# env.close()\n",
    "\n",
    "# Moving the Cart in One Direction\n",
    "env.step(0)\n",
    "\n",
    "# Observation Space Limits\n",
    "env.observation_space\n",
    "print(\"Observation Space -\", env.observation_space, \"\\n\")\n",
    "\n",
    "\n",
    "# Observation Space Upper Limit\n",
    "env.observation_space.high\n",
    "print(\"Observation Space (High Limit) -\", env.observation_space.high, \"\\n\")\n",
    "\n",
    "\n",
    "# Observation Space Lower Limit\n",
    "env.observation_space.low\n",
    "print(\"Observation Space (Low Limit) -\", env.observation_space.low, \"\\n\")\n",
    "\n",
    "# Action Space\n",
    "env.action_space\n",
    "print(\"Action Space -\", env.action_space, \"\\n\")\n",
    "\n",
    "# Review all specifications\n",
    "env.spec\n",
    "print(\"Specifications -\", env.spec, \"\\n\")\n",
    "\n",
    "# Max Number of Steps per Episode\n",
    "env.spec.max_episode_steps\n",
    "print(\"# of Steps per Episode -\", env.spec.max_episode_steps, \"\\n\")\n",
    "\n",
    "# Reward Threshold per Episode\n",
    "env.spec.reward_threshold\n",
    "print(\"Reward Threshhold -\", env.spec.reward_threshold, \"\\n\")\n",
    "\n",
    "# Simulating the Environment\n",
    "episodeNumber = 10000\n",
    "timeSteps = 100\n",
    "\n",
    "\"\"\" For each episode, the environment is reset,\n",
    "    the current episide index is printed to\n",
    "    the console, a new render is started, \n",
    "    observations are made, information on\n",
    "    the environment and movement is created,\n",
    "    and learning is perforemed by timesteps\n",
    "    and observations made based on actions.\n",
    "\"\"\"\n",
    "\n",
    "for episodeIndex in range(episodeNumber):\n",
    "    initial_state = env.reset()\n",
    "    print(episodeIndex)\n",
    "    env.render()\n",
    "    appendedObservations = []\n",
    "    for timeIndex in range(timeSteps):\n",
    "        print(timeIndex)\n",
    "        random_action = env.action_space.sample()\n",
    "        observation, reward, terminated, truncated, info = env.step(random_action)\n",
    "        appendedObservations.append(observation)\n",
    "        time.sleep(0.01)\n",
    "        if (terminated):\n",
    "            time.sleep(3)\n",
    "            break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56618837",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
